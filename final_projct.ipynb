{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyTorch's torchvision\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                3,\n",
    "                padding=dilation,\n",
    "                dilation=dilation,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = 256\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "class DeepLabHeadV3Plus(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=2048,\n",
    "        low_level_channels=256,\n",
    "        num_classes=43,\n",
    "        aspp_dilate=[12, 24, 36],\n",
    "        output_size=None,\n",
    "    ):\n",
    "        super(DeepLabHeadV3Plus, self).__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(304, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1),\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        low_level_feature = self.project(feature[\"low_level\"])\n",
    "        output_feature = self.aspp(feature[\"out\"])\n",
    "        output_size = (\n",
    "            self.output_size if self.output_size else low_level_feature.shape[2:]\n",
    "        )\n",
    "        low_level_feature = F.interpolate(\n",
    "            low_level_feature, size=output_size, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        output_feature = F.interpolate(\n",
    "            output_feature, size=output_size, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        return self.classifier(torch.cat([low_level_feature, output_feature], dim=1))\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PyTorch's torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "# __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "# \t\t   'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "# \t\t   'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes,\n",
    "        planes,\n",
    "        stride=1,\n",
    "        downsample=None,\n",
    "        groups=1,\n",
    "        base_width=64,\n",
    "        dilation=1,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers,\n",
    "        num_classes=1000,\n",
    "        zero_init_residual=False,\n",
    "        groups=1,\n",
    "        width_per_group=64,\n",
    "        replace_stride_with_dilation=None,\n",
    "        norm_layer=None,\n",
    "    ):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes,\n",
    "                planes,\n",
    "                stride,\n",
    "                downsample,\n",
    "                self.groups,\n",
    "                self.base_width,\n",
    "                previous_dilation,\n",
    "                norm_layer,\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "            pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "            progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicConv(nn.Module):  ## debug\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        dilation=1,\n",
    "        bias=False,\n",
    "        pooling=None,\n",
    "        groups=1,\n",
    "        batchnorm=True,\n",
    "        act=nn.ReLU(inplace=True),\n",
    "    ):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    out_planes,\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding,\n",
    "                    dilation,\n",
    "                    groups,\n",
    "                    bias,\n",
    "                ),  # training acceleration: bias off before BN\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if pooling:\n",
    "            self.layers.append(pooling)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.layers.append(nn.BatchNorm2d(out_planes))\n",
    "\n",
    "        if act:\n",
    "            self.layers.append(act)\n",
    "\n",
    "        self.sequential = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class ResNet_Proj(nn.Module):\n",
    "    def __init__(self, channels=(1024, 2048)):\n",
    "        super(ResNet_Proj, self).__init__()\n",
    "        layer2_channel, layer3_channel = channels\n",
    "        self.conv_bn_relu1 = BasicConv(\n",
    "            layer2_channel, layer2_channel, pooling=nn.AdaptiveAvgPool2d(24)\n",
    "        )\n",
    "        self.conv_bn_relu2 = BasicConv(\n",
    "            layer3_channel, layer3_channel, pooling=nn.AdaptiveAvgPool2d(12)\n",
    "        )\n",
    "\n",
    "    def forward(self, semantic_feat):\n",
    "        semantic_feat[2] = self.conv_bn_relu1(semantic_feat[2])\n",
    "        semantic_feat[3] = self.conv_bn_relu2(semantic_feat[3])\n",
    "        return semantic_feat\n",
    "\n",
    "\n",
    "class Res_DeepLabV3P(nn.Module):\n",
    "    \"\"\"\n",
    "    Res backbone for semantic feature extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(Res_DeepLabV3P, self).__init__()\n",
    "\n",
    "        self.resnet = resnet.__dict__[\"resnet50\"](\n",
    "            pretrained=True, replace_stride_with_dilation=[False, True, True]\n",
    "        )\n",
    "        self.conv1 = self.resnet.conv1\n",
    "        self.bn1 = self.resnet.bn1\n",
    "        self.relu = self.resnet.relu\n",
    "        self.max_pool = self.resnet.maxpool\n",
    "        self.layer1 = self.resnet.layer1\n",
    "        self.layer2 = self.resnet.layer2\n",
    "        self.layer3 = self.resnet.layer3\n",
    "        self.layer4 = self.resnet.layer4\n",
    "\n",
    "        self.projection = ResNet_Proj()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = []\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "        layer0 = x\n",
    "        x = self.layer1(x)\n",
    "        features.append(x)  # ; layer1 = x\n",
    "        x = self.layer2(x)\n",
    "        features.append(x)\n",
    "        x = self.layer3(x)\n",
    "        features.append(x)\n",
    "        x = self.layer4(x)\n",
    "        features.append(x)  # ; layer4 = x\n",
    "\n",
    "        features = self.projection(features)\n",
    "\n",
    "        return {\"backbone\": features, \"layer0\": layer0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# https://github.com/NVlabs/SegFormer/blob/master/mmseg/models/backbones/mix_transformer.py\n",
    "# Copyright (c) 2021, NVIDIA Corporation. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the NVIDIA Source Code License\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads=8,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "        sr_ratio=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            dim % num_heads == 0\n",
    "        ), f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .reshape(B, N, self.num_heads, C // self.num_heads)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x_ = self.norm(x_)\n",
    "            kv = (\n",
    "                self.kv(x_)\n",
    "                .reshape(B, -1, 2, self.num_heads, C // self.num_heads)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "        else:\n",
    "            kv = (\n",
    "                self.kv(x)\n",
    "                .reshape(B, -1, 2, self.num_heads, C // self.num_heads)\n",
    "                .permute(2, 0, 3, 1, 4)\n",
    "            )\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        sr_ratio=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "            sr_ratio=sr_ratio,\n",
    "        )\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
    "        self.num_patches = self.H * self.W\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=stride,\n",
    "            padding=(patch_size[0] // 2, patch_size[1] // 2),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, H, W\n",
    "\n",
    "\n",
    "class MixVisionTransformer(nn.Module):\n",
    "    official_ckpts = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        embed_dims=[64, 128, 256, 512],\n",
    "        num_heads=[1, 2, 4, 8],\n",
    "        mlp_ratios=[4, 4, 4, 4],\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        depths=[3, 4, 6, 3],\n",
    "        sr_ratios=[8, 4, 2, 1],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "\n",
    "        # patch_embed\n",
    "        self.patch_embed1 = OverlapPatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=7,\n",
    "            stride=4,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dims[0],\n",
    "        )\n",
    "        self.patch_embed2 = OverlapPatchEmbed(\n",
    "            img_size=img_size // 4,\n",
    "            patch_size=3,\n",
    "            stride=2,\n",
    "            in_chans=embed_dims[0],\n",
    "            embed_dim=embed_dims[1],\n",
    "        )\n",
    "        self.patch_embed3 = OverlapPatchEmbed(\n",
    "            img_size=img_size // 8,\n",
    "            patch_size=3,\n",
    "            stride=2,\n",
    "            in_chans=embed_dims[1],\n",
    "            embed_dim=embed_dims[2],\n",
    "        )\n",
    "        self.patch_embed4 = OverlapPatchEmbed(\n",
    "            img_size=img_size // 16,\n",
    "            patch_size=3,\n",
    "            stride=2,\n",
    "            in_chans=embed_dims[2],\n",
    "            embed_dim=embed_dims[3],\n",
    "        )\n",
    "\n",
    "        # transformer encoder\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "        self.block1 = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dims[0],\n",
    "                    num_heads=num_heads[0],\n",
    "                    mlp_ratio=mlp_ratios[0],\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[cur + i],\n",
    "                    norm_layer=norm_layer,\n",
    "                    sr_ratio=sr_ratios[0],\n",
    "                )\n",
    "                for i in range(depths[0])\n",
    "            ]\n",
    "        )\n",
    "        self.norm1 = norm_layer(embed_dims[0])\n",
    "\n",
    "        cur += depths[0]\n",
    "        self.block2 = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dims[1],\n",
    "                    num_heads=num_heads[1],\n",
    "                    mlp_ratio=mlp_ratios[1],\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[cur + i],\n",
    "                    norm_layer=norm_layer,\n",
    "                    sr_ratio=sr_ratios[1],\n",
    "                )\n",
    "                for i in range(depths[1])\n",
    "            ]\n",
    "        )\n",
    "        self.norm2 = norm_layer(embed_dims[1])\n",
    "\n",
    "        cur += depths[1]\n",
    "        self.block3 = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dims[2],\n",
    "                    num_heads=num_heads[2],\n",
    "                    mlp_ratio=mlp_ratios[2],\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[cur + i],\n",
    "                    norm_layer=norm_layer,\n",
    "                    sr_ratio=sr_ratios[2],\n",
    "                )\n",
    "                for i in range(depths[2])\n",
    "            ]\n",
    "        )\n",
    "        self.norm3 = norm_layer(embed_dims[2])\n",
    "\n",
    "        cur += depths[2]\n",
    "        self.block4 = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dims[3],\n",
    "                    num_heads=num_heads[3],\n",
    "                    mlp_ratio=mlp_ratios[3],\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[cur + i],\n",
    "                    norm_layer=norm_layer,\n",
    "                    sr_ratio=sr_ratios[3],\n",
    "                )\n",
    "                for i in range(depths[3])\n",
    "            ]\n",
    "        )\n",
    "        self.norm4 = norm_layer(embed_dims[3])\n",
    "\n",
    "        # classification head\n",
    "        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    # def init_weights(self, pretrained=None):\n",
    "    #     if isinstance(pretrained, str):\n",
    "    #         logger = get_root_logger()\n",
    "    #         load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n",
    "\n",
    "    def reset_drop_path(self, drop_path_rate):\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n",
    "        cur = 0\n",
    "        for i in range(self.depths[0]):\n",
    "            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[0]\n",
    "        for i in range(self.depths[1]):\n",
    "            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[1]\n",
    "        for i in range(self.depths[2]):\n",
    "            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[2]\n",
    "        for i in range(self.depths[3]):\n",
    "            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    def freeze_patch_emb(self):\n",
    "        self.patch_embed1.requires_grad = False\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\n",
    "            \"pos_embed1\",\n",
    "            \"pos_embed2\",\n",
    "            \"pos_embed3\",\n",
    "            \"pos_embed4\",\n",
    "            \"cls_token\",\n",
    "        }  # has pos_embed may be better\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=\"\"):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = (\n",
    "            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for i, blk in enumerate(self.block1):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for i, blk in enumerate(self.block2):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for i, blk in enumerate(self.block3):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for i, blk in enumerate(self.block4):\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        # x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_official_state_dict(\n",
    "        self,\n",
    "        filename: str,\n",
    "        local_dir: str = None,\n",
    "        download: bool = True,\n",
    "        strict: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            local_dir: if not None, load from \"local_dir/filename\"\n",
    "            strict: note that, the definition is silghtly different from load_state_dict().\n",
    "                    If set to flase, the weight of final layer will not be loaded, so num_classes can be any.\n",
    "        \"\"\"\n",
    "\n",
    "        assert (\n",
    "            filename in self.official_ckpts.keys()\n",
    "        ), f\"available checkpoints are {self.official_ckpts.keys()}\"\n",
    "\n",
    "        if local_dir is None:\n",
    "            local_dir = os.path.join(\n",
    "                os.path.expanduser(\"~\"), \".cache\", \"torch\", \"checkpoints\"\n",
    "            )\n",
    "\n",
    "        path = os.path.join(local_dir, filename)\n",
    "        if os.path.isfile(path):\n",
    "            ckpt = torch.load(path, map_location=\"cpu\")\n",
    "        elif download:\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            import gdown\n",
    "\n",
    "            url = self.official_ckpts[filename]\n",
    "            gdown.download(url, path, quiet=False)\n",
    "            ckpt = torch.load(path, map_location=\"cpu\")\n",
    "            # ckpt = load_state_dict_from_url(url, progress=True, file_name=filename, model_dir=local_dir)\n",
    "        else:\n",
    "            raise ValueError(\"You neither proivde local path nor set download True!\")\n",
    "\n",
    "        # state_dict = ckpt['state_dict']\n",
    "        # # exclude_keys = [\"decode_head.conv_seg.weight\", \"decode_head.conv_seg.bias\"]\n",
    "        # # if not strict:\n",
    "        # #     exclude_keys += [\"decode_head.linear_pred.weight\", \"decode_head.linear_pred.bias\"]\n",
    "        # exclude_keys = []\n",
    "        # ckpt_to_load = {k:v for k, v in state_dict.items() if k not in exclude_keys}\n",
    "        self.load_state_dict(ckpt, strict=strict)\n",
    "        print(\"loaded pretrained weight from\", path)\n",
    "\n",
    "    def reset_input_channel(self, new_in_chans, pretrained=True):\n",
    "        \"\"\"\n",
    "        this function can be used to change the input channels for a pretrained model.\n",
    "        the weights of first conv layer are cyclicaly copied.\n",
    "        see https://stackoverflow.com/questions/62629114/how-to-modify-resnet-50-with-4-channels-as-input-using-pre-trained-weights-in-py\n",
    "        and https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/encoders/_utils.py\n",
    "        \"\"\"\n",
    "        # pass\n",
    "        weight = self.patch_embed1.proj.weight.detach()\n",
    "        # bias = self.patch_embed1.proj.bias.detach()\n",
    "        embed_dim = self.patch_embed1.proj.out_channels\n",
    "        kernel_size = self.patch_embed1.proj.kernel_size\n",
    "        stride = self.patch_embed1.proj.stride\n",
    "        padding = self.patch_embed1.proj.padding\n",
    "        self.patch_embed1.proj = nn.Conv2d(\n",
    "            new_in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        if pretrained:\n",
    "            for ch in range(new_in_chans):\n",
    "                # self.patch_embed1.proj.bias.data[:, ch, :, :] = weight[:, ch%3, :, :]\n",
    "                self.patch_embed1.proj.weight.data[:, ch, :, :] = weight[\n",
    "                    :, ch % 3, :, :\n",
    "                ]\n",
    "\n",
    "\n",
    "class MiTB5(MixVisionTransformer):\n",
    "    official_ckpts = {\n",
    "        \"mit_b5.pth\": \"https://drive.google.com/uc?export=download&id=1d7I50jVjtCddnhpf-lqj8-f13UyCzoW1\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MiTB5, self).__init__(\n",
    "            patch_size=4,\n",
    "            embed_dims=[64, 128, 320, 512],\n",
    "            num_heads=[1, 2, 5, 8],\n",
    "            mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "            depths=[3, 6, 40, 3],\n",
    "            sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.1,\n",
    "        )\n",
    "\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, checkpoint_path=None, req_grad=False) -> None:\n",
    "        super(SegFormer, self).__init__()\n",
    "        self.segformer = MiTB5()\n",
    "        # if checkpoint_path:\n",
    "        # \tself.segformer.load_official_state_dict('mit_b5.pth', checkpoint_path)\n",
    "        # self.turn_grad(req_grad)\n",
    "\n",
    "    def turn_grad(self, req_grad):\n",
    "        if not req_grad:\n",
    "            for param in self.segformer.parameters():\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                print(\"Backbone SegFormer: turned off requires_grad\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        rgb_list = self.segformer(x)\n",
    "        return [rgb_list[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "\n",
    "## qkv prep\n",
    "class SqueezeAndExcitationSEShared(nn.Module):\n",
    "    def __init__(self, channel, activation=nn.ReLU(), channel_mid=None):\n",
    "        super(SqueezeAndExcitationSEShared, self).__init__()\n",
    "        if channel_mid is None:\n",
    "            channel_mid = channel // 2\n",
    "\n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n",
    "        self.conv1d1 = nn.Conv1d(channel, channel_mid, kernel_size=1)\n",
    "        self.act = activation\n",
    "        self.conv1d2 = nn.Conv1d(channel_mid, channel, kernel_size=1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, rgb, sem_encod):\n",
    "        weighting = self.conv1d1(self.avgpool1d(rgb))\n",
    "        if sem_encod is not None:\n",
    "            weighting = weighting + sem_encod\n",
    "\n",
    "        weighting = self.sigm(self.conv1d2(self.act(weighting)))\n",
    "\n",
    "        y = rgb * weighting\n",
    "        return y\n",
    "\n",
    "\n",
    "class Semantic_To_KV(nn.Module):\n",
    "    def __init__(self, embed_dim, semantic_assert):\n",
    "        super(Semantic_To_KV, self).__init__()\n",
    "        self.projection = SqueezeAndExcitationSEShared(\n",
    "            embed_dim, channel_mid=semantic_assert\n",
    "        )\n",
    "\n",
    "    def forward(self, semantic_feature, sem_encod):\n",
    "        sem = rearrange(\n",
    "            self.projection(rearrange(semantic_feature, \"hw b c -> b c hw\"), sem_encod),\n",
    "            \"b c hw -> hw b c\",\n",
    "        )\n",
    "        sem_k, sem_v = sem.chunk(2, dim=-1)\n",
    "\n",
    "        return sem_k, sem_v\n",
    "\n",
    "\n",
    "## Attention block\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.ff = PreNorm(embed_dim, FeedForward(embed_dim, 1024))\n",
    "\n",
    "    def forward(self, spatial_q, semantic_k, semantic_v):\n",
    "        attention, _ = self.attn(spatial_q, semantic_k, semantic_v, need_weights=False)\n",
    "        attention = self.norm(attention)\n",
    "        attention = self.ff(attention)\n",
    "        attention = attention + spatial_q\n",
    "        return attention\n",
    "\n",
    "\n",
    "# Context Correlation Attention (CCA) Module\n",
    "class CCA(nn.Module):\n",
    "    def __init__(self, spatial_dim, semantic_dim, transform_dim, semantic_assert):\n",
    "        super(CCA, self).__init__()\n",
    "\n",
    "        self.spatial_to_q = nn.Linear(spatial_dim, transform_dim)\n",
    "        self.semantic_to_kv = Semantic_To_KV(semantic_dim, semantic_assert)\n",
    "        self.attention = Attention(transform_dim)\n",
    "\n",
    "    def unflatten(self, x):\n",
    "        return rearrange(x, \"(h w) b c -> b c h w\", h=int(math.sqrt(x.shape[0])))\n",
    "\n",
    "    def forward(self, spatial_feature, semantic_feature, sem_encod):\n",
    "        spatial = rearrange(spatial_feature, \"b c h w -> (h w) b c\")\n",
    "        spatial_q = self.spatial_to_q(spatial)\n",
    "\n",
    "        semantic = rearrange(semantic_feature, \"b c h w -> (h w) b c\")\n",
    "        semantic_k, semantic_v = self.semantic_to_kv(semantic, sem_encod)\n",
    "\n",
    "        attended = self.attention(spatial_q, semantic_k, semantic_v)\n",
    "\n",
    "        return self.unflatten(attended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from model.backbone.ResNet import Res_DeepLabV3P\n",
    "from model.backbone.SegFormer import SegFormer\n",
    "from model.backbone.DeepLab import DeepLabHeadV3Plus\n",
    "from model.UperNet import UPerNet\n",
    "from model.SAA import SAA\n",
    "from model.CCA import CCA\n",
    "\n",
    "\n",
    "class Sem_Enc(nn.Module):\n",
    "    \"\"\"\n",
    "    Semantic Encoding Module.\n",
    "\n",
    "    This module converts semantic features from the ResNet backbone into encodings.\n",
    "    It processes the high-level semantic features using a DeepLabV3+ head followed by\n",
    "    a series of depthwise convolutions and pooling operations to generate encodings\n",
    "    of shape (B, num_classes, 1).\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of target classes for segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Sem_Enc module.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of target classes for segmentation.\n",
    "        \"\"\"\n",
    "        super(Sem_Enc, self).__init__()\n",
    "\n",
    "        # Initialize DeepLabV3+ head for projecting semantic features\n",
    "        self.projection: DeepLabHeadV3Plus = DeepLabHeadV3Plus(num_classes=num_classes)\n",
    "\n",
    "        # Depthwise convolution layers with specified kernel sizes and pooling\n",
    "        self.conv1: nn.Conv2d = nn.Conv2d(\n",
    "            in_channels=num_classes,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=4,\n",
    "            groups=num_classes,  # Depthwise convolution\n",
    "            bias=False,\n",
    "        )\n",
    "        self.pool1: nn.AvgPool2d = nn.AvgPool2d(kernel_size=6)\n",
    "\n",
    "        self.conv2: nn.Conv2d = nn.Conv2d(\n",
    "            in_channels=num_classes,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2,\n",
    "            groups=num_classes,  # Depthwise convolution\n",
    "            bias=False,\n",
    "        )\n",
    "        self.pool2: nn.AvgPool2d = nn.AvgPool2d(\n",
    "            kernel_size=4\n",
    "        )  # Reduces spatial dimensions from 16 to 4\n",
    "\n",
    "        self.conv3: nn.Conv2d = nn.Conv2d(\n",
    "            in_channels=num_classes,\n",
    "            out_channels=num_classes,\n",
    "            kernel_size=4,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=num_classes,  # Depthwise convolution\n",
    "            bias=False,\n",
    "        )  # Reduces spatial dimensions from 4 to 1\n",
    "\n",
    "        # Batch normalization and ReLU activation\n",
    "        self.bn: nn.BatchNorm2d = nn.BatchNorm2d(num_classes)\n",
    "        self.relu: nn.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Sem_Enc module.\n",
    "\n",
    "        Args:\n",
    "            features (List[torch.Tensor]): List of feature maps from the ResNet backbone.\n",
    "                                           Expected to contain [layer1, layer2, layer3, layer4].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Semantic encodings with shape (B, num_classes).\n",
    "        \"\"\"\n",
    "        # Apply DeepLabV3+ projection to semantic features\n",
    "        # Input: {'low_level': features[0], 'out': features[3]}\n",
    "        x: torch.Tensor = self.projection(\n",
    "            {\"low_level\": features[0], \"out\": features[3]}\n",
    "        )  # Shape: (B, num_classes, H, W)\n",
    "\n",
    "        # First convolution and pooling layer\n",
    "        conv: torch.Tensor = self.conv1(x)  # Shape: (B, num_classes, H, W)\n",
    "        conv = self.pool1(conv)  # Shape: (B, num_classes, H//6, W//6)\n",
    "        conv = self.bn(conv)  # Shape: (B, num_classes, H//6, W//6)\n",
    "        conv = self.relu(conv)  # Shape: (B, num_classes, H//6, W//6)\n",
    "\n",
    "        # Second convolution and pooling layer\n",
    "        conv = self.conv2(conv)  # Shape: (B, num_classes, H//6, W//6)\n",
    "        conv = self.pool2(conv)  # Shape: (B, num_classes, H//24, W//24)\n",
    "        conv = self.bn(conv)  # Shape: (B, num_classes, H//24, W//24)\n",
    "        conv = self.relu(conv)  # Shape: (B, num_classes, H//24, W//24)\n",
    "\n",
    "        # Third convolution layer to reduce spatial dimensions to 1x1\n",
    "        conv = self.conv3(conv)  # Shape: (B, num_classes, 1, 1)\n",
    "        conv = self.bn(conv)  # Shape: (B, num_classes, 1, 1)\n",
    "        conv = self.relu(conv)  # Shape: (B, num_classes, 1, 1)\n",
    "\n",
    "        # Squeeze the last two dimensions to obtain (B, num_classes)\n",
    "        return conv.squeeze(3)  # Shape: (B, num_classes, 1) -> (B, num_classes)\n",
    "\n",
    "\n",
    "class GlassSemNet(nn.Module):\n",
    "    \"\"\"\n",
    "    GlassSemNet Model for Semantic Segmentation.\n",
    "\n",
    "    This model integrates spatial and semantic backbones, semantic encodings,\n",
    "    Scene Aware Activation (SAA) modules, Context Correlation Attention (CCA) module,\n",
    "    and a decoder (UPerNet) to produce segmentation outputs.\n",
    "\n",
    "    Architecture Components:\n",
    "        - Spatial Backbone: SegFormer\n",
    "        - Semantic Backbone: Res_DeepLabV3P\n",
    "        - Semantic Encoding: Sem_Enc\n",
    "        - SAA Modules: SAA0, SAA1, SAA2\n",
    "        - CCA Module: CCA3\n",
    "        - Decoder: UPerNet\n",
    "        - Auxiliary Outputs: aux1, aux2\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GlassSemNet model.\n",
    "        \"\"\"\n",
    "        super(GlassSemNet, self).__init__()\n",
    "\n",
    "        # Define the number of segmentation classes\n",
    "        self.num_classes: int = 43\n",
    "\n",
    "        # Initialize the spatial backbone (SegFormer)\n",
    "        self.spatial_backbone: SegFormer = SegFormer()\n",
    "\n",
    "        # Initialize the semantic backbone (ResNet with DeepLabV3+)\n",
    "        self.semantic_backbone: Res_DeepLabV3P = Res_DeepLabV3P()\n",
    "\n",
    "        # Initialize the semantic encoding module\n",
    "        self.sem_enc: Sem_Enc = Sem_Enc(num_classes=self.num_classes)\n",
    "\n",
    "        # Initialize Scene Aware Activation (SAA) modules for different feature levels\n",
    "        self.saa0: SAA = SAA(\n",
    "            spatial_dim=64, semantic_dim=256, semantic_assert=self.num_classes\n",
    "        )\n",
    "        self.saa1: SAA = SAA(\n",
    "            spatial_dim=128, semantic_dim=512, semantic_assert=self.num_classes\n",
    "        )\n",
    "        self.saa2: SAA = SAA(\n",
    "            spatial_dim=320, semantic_dim=1024, semantic_assert=self.num_classes\n",
    "        )\n",
    "\n",
    "        # Initialize Context Correlation Attention (CCA) module for the highest feature level\n",
    "        self.cca3: CCA = CCA(\n",
    "            spatial_dim=512,\n",
    "            semantic_dim=2048,\n",
    "            transform_dim=1024,\n",
    "            semantic_assert=self.num_classes,\n",
    "        )\n",
    "\n",
    "        # Initialize auxiliary convolution layers for intermediate outputs (optional)\n",
    "        self.aux1: nn.Conv2d = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1)\n",
    "        self.aux2: nn.Conv2d = nn.Conv2d(\n",
    "            in_channels=1024, out_channels=1, kernel_size=1\n",
    "        )\n",
    "\n",
    "        # Initialize the decoder (UPerNet) for final segmentation output\n",
    "        self.decoder: UPerNet = UPerNet()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the GlassSemNet model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (B, 3, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Segmentation output tensor with shape (B, num_class, H', W'),\n",
    "                          where (H', W') is determined by the decoder's output resolution.\n",
    "        \"\"\"\n",
    "        # ---------------------------\n",
    "        # Spatial Backbone Forward Pass\n",
    "        # ---------------------------\n",
    "        spatial_feats: List[torch.Tensor] = self.spatial_backbone(x)\n",
    "        # spatial_feats: List of feature maps from SegFormer, e.g., [P2, P3, P4, P5]\n",
    "\n",
    "        # ---------------------------\n",
    "        # Semantic Backbone Forward Pass\n",
    "        # ---------------------------\n",
    "        resnet_out: Dict[str, List[torch.Tensor]] = self.semantic_backbone(x)\n",
    "        semantic_feats: List[torch.Tensor] = resnet_out[\"backbone\"]\n",
    "        semantic_lowlevel: torch.Tensor = resnet_out[\"layer0\"]\n",
    "        # semantic_feats: List of semantic feature maps from ResNet, e.g., [C1, C2, C3, C4]\n",
    "\n",
    "        # ---------------------------\n",
    "        # Semantic Encodings\n",
    "        # ---------------------------\n",
    "        sem_enc: torch.Tensor = self.sem_enc(semantic_feats)\n",
    "        # sem_enc: Semantic encodings with shape (B, num_classes)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Scene Aware Activation (SAA) Modules\n",
    "        # ---------------------------\n",
    "        saa0: torch.Tensor = self.saa0(\n",
    "            spatial_feature=spatial_feats[0],\n",
    "            semantic_feature=semantic_feats[0],\n",
    "            sem_encod=sem_enc,\n",
    "        )\n",
    "        saa1: torch.Tensor = self.saa1(\n",
    "            spatial_feature=spatial_feats[1],\n",
    "            semantic_feature=semantic_feats[1],\n",
    "            sem_encod=sem_enc,\n",
    "        )\n",
    "        saa2: torch.Tensor = self.saa2(\n",
    "            spatial_feature=spatial_feats[2],\n",
    "            semantic_feature=semantic_feats[2],\n",
    "            sem_encod=sem_enc,\n",
    "        )\n",
    "\n",
    "        # ---------------------------\n",
    "        # Context Correlation Attention (CCA) Module\n",
    "        # ---------------------------\n",
    "        cca3: torch.Tensor = self.cca3(\n",
    "            spatial_feature=spatial_feats[3],\n",
    "            semantic_feature=semantic_feats[3],\n",
    "            sem_encod=sem_enc,\n",
    "        )\n",
    "\n",
    "        # ---------------------------\n",
    "        # Decoder Preparation\n",
    "        # ---------------------------\n",
    "        # Concatenate spatial, semantic, and activated features for each level\n",
    "        l0: torch.Tensor = torch.cat(\n",
    "            [spatial_feats[0], semantic_feats[0], saa0], dim=1\n",
    "        )  # Shape: (B, C0 + C0 + C0, H0, W0)\n",
    "        l1: torch.Tensor = torch.cat(\n",
    "            [spatial_feats[1], semantic_feats[1], saa1], dim=1\n",
    "        )  # Shape: (B, C1 + C1 + C1, H1, W1)\n",
    "        l2: torch.Tensor = torch.cat(\n",
    "            [spatial_feats[2], semantic_feats[2], saa2], dim=1\n",
    "        )  # Shape: (B, C2 + C2 + C2, H2, W2)\n",
    "        l3: torch.Tensor = torch.cat(\n",
    "            [spatial_feats[3], semantic_feats[3], cca3], dim=1\n",
    "        )  # Shape: (B, C3 + C3 + C3, H3, W3)\n",
    "\n",
    "        # Prepare the list of feature maps for the decoder\n",
    "        # Typically, decoder expects [low_level, P2, P3, P4, P5]\n",
    "        decoder_feats: List[torch.Tensor] = [semantic_lowlevel, l0, l1, l2, l3]\n",
    "\n",
    "        # ---------------------------\n",
    "        # Decoder Forward Pass\n",
    "        # ---------------------------\n",
    "        out: torch.Tensor = self.decoder(decoder_feats)\n",
    "        # out: Segmentation output from UPerNet, shape depends on UPerNet configuration\n",
    "\n",
    "        # Optional: Auxiliary outputs (commented out)\n",
    "        # aux_out1: torch.Tensor = self.aux1(saa1)  # Shape: (B, 1, H1, W1)\n",
    "        # aux_out2: torch.Tensor = self.aux2(cca3)  # Shape: (B, 1, H3, W3)\n",
    "        # out = out + aux_out1 + aux_out2  # Combine main and auxiliary outputs\n",
    "\n",
    "        return out  # Final segmentation output\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# if __name__ == '__main__':\n",
    "#     # Create a random input tensor with batch size 2, 3 channels, and 384x384 spatial dimensions\n",
    "#     x: torch.Tensor = torch.rand(2, 3, 384, 384)\n",
    "\n",
    "#     # Initialize the GlassSemNet model\n",
    "#     model: GlassSemNet = GlassSemNet()\n",
    "\n",
    "#     # Perform a forward pass\n",
    "#     out: torch.Tensor = model(x)\n",
    "\n",
    "#     # Print the output shape\n",
    "#     print(out.shape)  # Expected shape: (2, num_class, H', W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model.backbone.ResNet import BasicConv\n",
    "\n",
    "\n",
    "# CBAM\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, compressed_channels=None):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        if compressed_channels is None:\n",
    "            compressed_channels = gate_channels // 2\n",
    "        self.flat = Flatten()\n",
    "        self.lin1 = nn.Linear(gate_channels, compressed_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(compressed_channels, gate_channels)\n",
    "\n",
    "    def forward(self, x, sem_encod):\n",
    "        avg_pool = F.avg_pool2d(\n",
    "            x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3))\n",
    "        )\n",
    "        channel_att_sum = self.lin1(self.flat(avg_pool))\n",
    "        if sem_encod is not None:\n",
    "            channel_att_sum = channel_att_sum + sem_encod\n",
    "\n",
    "        channel_att_sum = self.lin2(self.relu(channel_att_sum))\n",
    "        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        GAP = torch.mean(x, dim=1, keepdim=True)\n",
    "        GMP, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        return torch.cat([GAP, GMP], dim=1)\n",
    "\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(\n",
    "            2,\n",
    "            1,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            act=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class CBAMResidualShared(nn.Module):\n",
    "\n",
    "    def __init__(self, spatial_dim, semantic_dim, semantic_assert):\n",
    "        super(CBAMResidualShared, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(semantic_dim, semantic_assert)\n",
    "        self.SpatialGate = SpatialGate()\n",
    "        self.ConvFusion = nn.Conv2d(spatial_dim + semantic_dim, semantic_dim, 1)\n",
    "        self.ConvAttentFusion = nn.Conv2d(spatial_dim + semantic_dim, semantic_dim, 1)\n",
    "\n",
    "    def forward(self, spatial, semantic, sem_encod):\n",
    "        spatial_ = self.SpatialGate(spatial)\n",
    "        semantic_ = self.ChannelGate(\n",
    "            semantic, sem_encod.squeeze(2)\n",
    "        )  # [b, c] + [b, c, 1] => [b, c] + [b, c]\n",
    "\n",
    "        feature_sum = spatial_ + semantic_\n",
    "        feature_prod = spatial_ * semantic_\n",
    "\n",
    "        x_out = self.ConvFusion(torch.cat([spatial, semantic], 1))\n",
    "        x_att = self.ConvAttentFusion(torch.cat([feature_sum, feature_prod], 1))\n",
    "\n",
    "        return x_out + x_att\n",
    "\n",
    "\n",
    "# Scene Aware Activation (SAA) Module\n",
    "class SAA(nn.Module):\n",
    "    def __init__(self, spatial_dim, semantic_dim, semantic_assert):\n",
    "        super(SAA, self).__init__()\n",
    "        self.projection = nn.Conv2d(spatial_dim, semantic_dim, 1)\n",
    "        self.cbam = CBAMResidualShared(semantic_dim, semantic_dim, semantic_assert)\n",
    "\n",
    "    def forward(self, spatial_feature, semantic_feature, sem_encod):\n",
    "        spatial_feature_proj = self.projection(spatial_feature)\n",
    "        activated = self.cbam(spatial_feature_proj, semantic_feature, sem_encod)\n",
    "\n",
    "        return activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model.backbone.ResNet import BasicConv\n",
    "\n",
    "\n",
    "class UPerNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_class=1,\n",
    "        use_softmax=False,\n",
    "        pool_scales=(1, 2, 3, 6),\n",
    "        fpn_inplanes=(64, 576, 1152, 2368, 3584),\n",
    "        fpn_dim=512,\n",
    "    ):\n",
    "        super(UPerNet, self).__init__()\n",
    "        self.use_softmax = use_softmax\n",
    "\n",
    "        # PPM Module\n",
    "        self.ppm_pooling = []\n",
    "        self.ppm_conv = []\n",
    "\n",
    "        for scale in pool_scales:\n",
    "            self.ppm_pooling.append(nn.AdaptiveAvgPool2d(scale))\n",
    "            self.ppm_conv.append(\n",
    "                BasicConv(fpn_inplanes[-1], 512, kernel_size=1, padding=0)\n",
    "            )\n",
    "        self.ppm_pooling = nn.ModuleList(self.ppm_pooling)\n",
    "        self.ppm_conv = nn.ModuleList(self.ppm_conv)\n",
    "        self.ppm_last_conv = BasicConv(\n",
    "            fpn_inplanes[-1] + len(pool_scales) * 512, fpn_dim, padding=0\n",
    "        )\n",
    "\n",
    "        # FPN Module\n",
    "        self.fpn_in = []\n",
    "        for fpn_inplane in fpn_inplanes[:-1]:  # skip the top layer\n",
    "            self.fpn_in.append(BasicConv(fpn_inplane, fpn_dim, kernel_size=1))\n",
    "        self.fpn_in = nn.ModuleList(self.fpn_in)\n",
    "\n",
    "        self.fpn_out = []\n",
    "        for i in range(len(fpn_inplanes) - 1):  # skip the top layer\n",
    "            self.fpn_out.append(\n",
    "                nn.Sequential(\n",
    "                    BasicConv(fpn_dim, fpn_dim),\n",
    "                )\n",
    "            )\n",
    "        self.fpn_out = nn.ModuleList(self.fpn_out)\n",
    "\n",
    "        self.conv_last = nn.Sequential(\n",
    "            BasicConv(len(fpn_inplanes) * fpn_dim, fpn_dim),\n",
    "            nn.Conv2d(fpn_dim, num_class, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, conv_out):\n",
    "        conv5 = conv_out[-1]\n",
    "\n",
    "        input_size = conv5.size()\n",
    "        ppm_out = [conv5]\n",
    "        for pool_scale, pool_conv in zip(self.ppm_pooling, self.ppm_conv):\n",
    "            ppm_out.append(\n",
    "                pool_conv(\n",
    "                    nn.functional.interpolate(\n",
    "                        pool_scale(conv5),\n",
    "                        (input_size[2], input_size[3]),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=False,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ppm_out = torch.cat(ppm_out, 1)\n",
    "        f = self.ppm_last_conv(ppm_out)\n",
    "\n",
    "        fpn_feature_list = [f]\n",
    "        for i in reversed(range(len(conv_out) - 1)):\n",
    "            conv_x = conv_out[i]\n",
    "            conv_x = self.fpn_in[i](conv_x)  # lateral branch\n",
    "\n",
    "            f = nn.functional.interpolate(\n",
    "                f, size=conv_x.size()[2:], mode=\"bilinear\", align_corners=False\n",
    "            )  # top-down branch\n",
    "            f = conv_x + f\n",
    "\n",
    "            fpn_feature_list.append(self.fpn_out[i](f))\n",
    "\n",
    "        fpn_feature_list.reverse()  # [P2 - P5]\n",
    "        output_size = fpn_feature_list[0].size()[2:]\n",
    "        fusion_list = [fpn_feature_list[0]]\n",
    "        for i in range(1, len(fpn_feature_list)):\n",
    "            fusion_list.append(\n",
    "                nn.functional.interpolate(\n",
    "                    fpn_feature_list[i],\n",
    "                    output_size,\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "                )\n",
    "            )\n",
    "        fusion_out = torch.cat(fusion_list, 1)\n",
    "        x = self.conv_last(fusion_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "\n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def crf_refine(img, annos):\n",
    "    assert img.dtype == np.uint8\n",
    "    assert annos.dtype == np.uint8\n",
    "    assert img.shape[:2] == annos.shape\n",
    "\n",
    "    # {img, annos}: {np.array(uint8)}\n",
    "\n",
    "    EPSILON = 1e-8\n",
    "\n",
    "    M = 2\n",
    "    tau = 1.05\n",
    "    # CRF model setup\n",
    "    d = dcrf.DenseCRF2D(img.shape[1], img.shape[0], M)\n",
    "\n",
    "    anno_norm = annos / 255.0\n",
    "\n",
    "    n_energy = -np.log((1.0 - anno_norm + EPSILON)) / (tau * _sigmoid(1 - anno_norm))\n",
    "    p_energy = -np.log(anno_norm + EPSILON) / (tau * _sigmoid(anno_norm))\n",
    "\n",
    "    U = np.zeros((M, img.shape[0] * img.shape[1]), dtype=\"float32\")\n",
    "    U[0, :] = n_energy.flatten()\n",
    "    U[1, :] = p_energy.flatten()\n",
    "\n",
    "    d.setUnaryEnergy(U)\n",
    "\n",
    "    d.addPairwiseGaussian(sxy=3, compat=3)\n",
    "    d.addPairwiseBilateral(sxy=60, srgb=5, rgbim=img, compat=5)\n",
    "\n",
    "    # Inference\n",
    "    infer = np.array(d.inference(1)).astype(\"float32\")\n",
    "    res = infer[1, :]\n",
    "\n",
    "    res = res * 255\n",
    "    res = res.reshape(img.shape[:2])\n",
    "    return res.astype(\"uint8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
